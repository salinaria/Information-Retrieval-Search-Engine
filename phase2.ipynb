{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import json file and read it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'اعلام زمان قرعه کشی جام باشگاه های فوتسال آسیا', 'content': '\\nبه گزارش خبرگزاری فارس، کنفدراسیون فوتبال آسیا (AFC) در نامه ای رسمی به فدراسیون فوتبال ایران و باشگاه گیتی پسند زمان\\xa0 قرعه کشی جام باشگاه های فوتسال آسیا را رسماً اعلام کرد. بر این اساس 25 فروردین ماه 1401 مراسم قرعه کشی جام باشگاه های فوتسال آسیا در مالزی برگزار می شود. باشگاه گیتی پسند بعنوان قهرمان فوتسال ایران در سال 1400 به این مسابقات راه پیدا کرده است. پیش از این گیتی پسند تجربه 3 دوره حضور در جام باشگاه های فوتسال آسیا را داشته که هر سه دوره به فینال مسابقات راه پیدا کرده و یک عنوان قهرمانی و دو مقام دومی بدست آورده است. انتهای پیام/\\n\\n\\n', 'tags': ['اعلام زمان', 'قرعه\\u200cکشی', 'قرعه\\u200cکشی جام', 'قرعه\\u200cکشی جام باشگاه\\u200cهای فوتسال', 'ای اف سی', 'گیتی پسند'], 'date': '3/15/2022 5:59:27 PM', 'url': 'https://www.farsnews.ir/news/14001224001005/اعلام-زمان-قرعه-کشی-جام-باشگاه-های-فوتسال-آسیا', 'category': 'sports'}\n",
      "12202\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "f = open('IR_data_news_12k.json')\n",
    "data = json.load(f)\n",
    "print(data['0'])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nبه گزارش خبرگزاری فارس، کنفدراسیون فوتبال آسیا (AFC) در نامه ای رسمی به فدراسیون فوتبال ایران و باشگاه گیتی پسند زمان\\xa0 قرعه کشی جام باشگاه های فوتسال آسیا را رسماً اعلام کرد. بر این اساس 25 فروردین ماه 1401 مراسم قرعه کشی جام باشگاه های فوتسال آسیا در مالزی برگزار می شود. باشگاه گیتی پسند بعنوان قهرمان فوتسال ایران در سال 1400 به این مسابقات راه پیدا کرده است. پیش از این گیتی پسند تجربه 3 دوره حضور در جام باشگاه های فوتسال آسیا را داشته که هر سه دوره به فینال مسابقات راه پیدا کرده و یک عنوان قهرمانی و دو مقام دومی بدست آورده است. انتهای پیام/\\n\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['0']['content']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-proccesing\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing data and normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['به', 'گزارش', 'خبرگزاری', 'فارس', '،', 'کنفدراسیون', 'فوتبال', 'آسیا', '(', 'AFC', ')', 'در', 'نامه\\u200cای', 'رسمی', 'به', 'فدراسیون', 'فوتبال', 'ایران', 'و', 'باشگاه', 'گیتی', 'پسند', 'زمان', 'قرعه', 'کشی', 'جام', 'باشگاه\\u200cهای', 'فوتسال', 'آسیا', 'را', 'رسما', 'اعلام', 'کرد', '.', 'بر', 'این', 'اساس', '25', 'فروردین', 'ماه', '1401', 'مراسم', 'قرعه', 'کشی', 'جام', 'باشگاه\\u200cهای', 'فوتسال', 'آسیا', 'در', 'مالزی', 'برگزار', 'می\\u200cشود', '.', 'باشگاه', 'گیتی', 'پسند', 'بعنوان', 'قهرمان', 'فوتسال', 'ایران', 'در', 'سال', '1400', 'به', 'این', 'مسابقات', 'راه', 'پیدا', 'کرده', 'است', '.', 'پیش', 'از', 'این', 'گیتی', 'پسند', 'تجربه', '3', 'دوره', 'حضور', 'در', 'جام', 'باشگاه\\u200cهای', 'فوتسال', 'آسیا', 'را', 'داشته', 'که', 'هر', 'سه', 'دوره', 'به', 'فینال', 'مسابقات', 'راه', 'پیدا', 'کرده', 'و', 'یک', 'عنوان', 'قهرمانی', 'و', 'دو', 'مقام', 'دومی', 'بدست', 'آورده', 'است', '.', 'انتهای', 'پیام', '/']\n"
     ]
    }
   ],
   "source": [
    "#!pip install parsivar\n",
    "from parsivar import Tokenizer\n",
    "from parsivar import Normalizer\n",
    "\n",
    "my_normalizer = Normalizer()\n",
    "my_tokenizer = Tokenizer()\n",
    "words = my_tokenizer.tokenize_words(\n",
    "    my_normalizer.normalize(data['0']['content']))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12202\n",
      "['به', 'گزارش', 'خبرگزاری', 'فارس', '،', 'کنفدراسیون', 'فوتبال', 'آسیا', '(', 'AFC', ')', 'در', 'نامه\\u200cای', 'رسمی', 'به', 'فدراسیون', 'فوتبال', 'ایران', 'و', 'باشگاه', 'گیتی', 'پسند', 'زمان', 'قرعه', 'کشی', 'جام', 'باشگاه\\u200cهای', 'فوتسال', 'آسیا', 'را', 'رسما', 'اعلام', 'کرد', '.', 'بر', 'این', 'اساس', '25', 'فروردین', 'ماه', '1401', 'مراسم', 'قرعه', 'کشی', 'جام', 'باشگاه\\u200cهای', 'فوتسال', 'آسیا', 'در', 'مالزی', 'برگزار', 'می\\u200cشود', '.', 'باشگاه', 'گیتی', 'پسند', 'بعنوان', 'قهرمان', 'فوتسال', 'ایران', 'در', 'سال', '1400', 'به', 'این', 'مسابقات', 'راه', 'پیدا', 'کرده', 'است', '.', 'پیش', 'از', 'این', 'گیتی', 'پسند', 'تجربه', '3', 'دوره', 'حضور', 'در', 'جام', 'باشگاه\\u200cهای', 'فوتسال', 'آسیا', 'را', 'داشته', 'که', 'هر', 'سه', 'دوره', 'به', 'فینال', 'مسابقات', 'راه', 'پیدا', 'کرده', 'و', 'یک', 'عنوان', 'قهرمانی', 'و', 'دو', 'مقام', 'دومی', 'بدست', 'آورده', 'است', '.', 'انتهای', 'پیام', '/']\n"
     ]
    }
   ],
   "source": [
    "tokenized = {}\n",
    "\n",
    "for i in data:\n",
    "    words = my_tokenizer.tokenize_words(\n",
    "        my_normalizer.normalize(data[i]['content']))\n",
    "    tokenized[i] = words\n",
    "\n",
    "print(len(tokenized))\n",
    "print(tokenized['0'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['و', 'در', 'به', 'از', 'که', 'این', 'را', 'با', 'است', 'برای']\n",
      "389\n"
     ]
    }
   ],
   "source": [
    "#!pip install hazm\n",
    "from hazm import *\n",
    "\n",
    "stop_words = stopwords_list()\n",
    "print(stop_words[:10])\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = ['.', '،', '/', ':', '(', ')', '!', '?', '؟']\n",
    "\n",
    "def del_stop_words(tok_data):\n",
    "    deleted_stop_words = {}\n",
    "    deleted = 0\n",
    "    all = 0\n",
    "    for i in tok_data:\n",
    "        text = []\n",
    "        for j in tok_data[i]:\n",
    "            if j not in stop_words and j not in punctuations:\n",
    "                text.append(j)\n",
    "                all += 1\n",
    "            else:\n",
    "                deleted += 1\n",
    "        deleted_stop_words[i] = text\n",
    "\n",
    "    print('stop words : ' + str(deleted))\n",
    "    print('remained : ' + str(all))\n",
    "\n",
    "    return deleted_stop_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "خبرنگاران خبرنگار\n",
      "داشت داشت&دار\n",
      "هستیم هستی\n",
      "خواهیم خواست&خواه\n",
      "تماشاگران تماشاگر\n",
      "داشت داشت&دار\n",
      "تماشاگران تماشاگر\n",
      "است اس\n"
     ]
    }
   ],
   "source": [
    "from parsivar import FindStems\n",
    "\n",
    "myStem = FindStems()\n",
    "for i in tokenized['1']:\n",
    "    if i != myStem.convert_to_stem(i):\n",
    "        print(i, myStem.convert_to_stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tok_data):\n",
    "    stemmed = {}\n",
    "\n",
    "    for i in tok_data:\n",
    "        stem_sent = []\n",
    "        for j in tok_data[i]:\n",
    "            new_word = j\n",
    "            stem = myStem.convert_to_stem(new_word)\n",
    "            stem_sent.append(stem)\n",
    "        stemmed[i] = stem_sent\n",
    "    return stemmed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words : 2215031\n",
      "remained : 2643556\n"
     ]
    }
   ],
   "source": [
    "deleted = del_stop_words(tokenized)\n",
    "processed = stemming(deleted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional inverted index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, {'7346': (2, [9, 104]), '7395': (1, [18]), '7416': (1, [25]), '7535': (1, [9]), '7579': (1, [954]), '7634': (1, [1683]), '8125': (4, [23, 34, 60, 110]), '8463': (2, [882, 913]), '8645': (1, [20]), '9261': (1, [41]), '9286': (5, [823, 1012, 1045, 1249, 1265]), '9808': (1, [74]), '9845': (1, [1281]), '10024': (2, [83, 105]), '10361': (1, [69]), '10567': (3, [126, 150, 165]), '10568': (2, [630, 714]), '10681': (1, [21]), '11176': (1, [103]), '11405': (3, [64, 1074, 1152]), '11437': (1, [3311]), '11603': (6, [18, 26, 53, 91, 155, 210]), '11645': (1, [18]), '11694': (1, [233]), '11928': (1, [2613]), '12134': (1, [335])})\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "dictionary = {}\n",
    "\n",
    "for doc in processed:\n",
    "    for term_index in range(len(processed[doc])):\n",
    "        if processed[doc][term_index] not in dictionary:\n",
    "            docs = {}\n",
    "            term_indexes = []\n",
    "            term_indexes.append(term_index)\n",
    "            docs[doc] = (1, term_indexes)\n",
    "            dictionary[processed[doc][term_index]] = (1, docs)\n",
    "        else:\n",
    "            num_all = dictionary[processed[doc][term_index]][0]\n",
    "            docs = dictionary[processed[doc][term_index]][1]\n",
    "\n",
    "            if doc in docs:\n",
    "                num_doc = docs[doc][0]\n",
    "                term_indexes = docs[doc][1]\n",
    "                term_indexes.append(term_index)\n",
    "                docs[doc] = (num_doc + 1, term_indexes)\n",
    "            else:\n",
    "                term_indexes = []\n",
    "                term_indexes.append(term_index)\n",
    "                docs[doc] = (1, term_indexes)\n",
    "                dictionary[processed[doc][term_index]] = (num_all + 1, docs)\n",
    "\n",
    "\n",
    "print(dictionary['امیرکبیر'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, {'7346': (2, [9, 104]), '7395': (1, [18]), '7416': (1, [25]), '7535': (1, [9]), '7579': (1, [954]), '7634': (1, [1683]), '8125': (4, [23, 34, 60, 110]), '8463': (2, [882, 913]), '8645': (1, [20]), '9261': (1, [41]), '9286': (5, [823, 1012, 1045, 1249, 1265]), '9808': (1, [74]), '9845': (1, [1281]), '10024': (2, [83, 105]), '10361': (1, [69]), '10567': (3, [126, 150, 165]), '10568': (2, [630, 714]), '10681': (1, [21]), '11176': (1, [103]), '11405': (3, [64, 1074, 1152]), '11437': (1, [3311]), '11603': (6, [18, 26, 53, 91, 155, 210]), '11645': (1, [18]), '11694': (1, [233]), '11928': (1, [2613]), '12134': (1, [335])})\n",
      "(2.6714576726855506, {'7346': (3.4756465643105914, [9, 104]), '7395': (2.6714576726855506, [18]), '7416': (2.6714576726855506, [25]), '7535': (2.6714576726855506, [9]), '7579': (2.6714576726855506, [954]), '7634': (2.6714576726855506, [1683]), '8125': (4.279835455935632, [23, 34, 60, 110]), '8463': (3.4756465643105914, [882, 913]), '8645': (2.6714576726855506, [20]), '9261': (2.6714576726855506, [41]), '9286': (4.538726453746061, [823, 1012, 1045, 1249, 1265]), '9808': (2.6714576726855506, [74]), '9845': (2.6714576726855506, [1281]), '10024': (3.4756465643105914, [83, 105]), '10361': (2.6714576726855506, [69]), '10567': (3.9460669094077496, [126, 150, 165]), '10568': (3.4756465643105914, [630, 714]), '10681': (2.6714576726855506, [21]), '11176': (2.6714576726855506, [103]), '11405': (3.9460669094077496, [64, 1074, 1152]), '11437': (2.6714576726855506, [3311]), '11603': (4.7502558010327895, [18, 26, 53, 91, 155, 210]), '11645': (2.6714576726855506, [18]), '11694': (2.6714576726855506, [233]), '11928': (2.6714576726855506, [2613]), '12134': (2.6714576726855506, [335])})\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "dic_tf = copy.deepcopy(dictionary)\n",
    "for term in dic_tf:\n",
    "    idf = math.log(12202/dic_tf[term][0],10)\n",
    "    dic_tf[term] = (idf,dic_tf[term][1])\n",
    "    for doc in dic_tf[term][1]:\n",
    "        dic_tf[term][1][doc] = (idf * (1+ math.log(dic_tf[term][1][doc][0],10)), dic_tf[term][1][doc][1])\n",
    "\n",
    "print(dictionary['امیرکبیر'])\n",
    "print(dic_tf['امیرکبیر'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query retrieval\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mتجمع دانشجویان در فرودگاه امام| تاکید بر رعایت شروط رهبر انقلاب در مذاکرات\u001b[0m\n",
      "به گزارش خبرنگار تشکل‌های دانشگاهی خبرگزاری فارس، جمعی از دانشجویان بسیجی دانشگاه‌های امیرکبیر و علامه‌طباطبایی همزمان با اعزام علی باقری رئیس تیم مذاکره کننده هسته‌ای کشورمان به وین، در فرودگاه امام خمینی(ره) تجمع کردند .\n",
      "\n",
      "\u001b[1mاعلام زمان قرعه کشی جام باشگاه های فوتسال آسیا\u001b[0m\n",
      "به گزارش خبرگزاری فارس، کنفدراسیون فوتبال آسیا (AFC) در نامه ای رسمی به فدراسیون فوتبال ایران و باشگاه گیتی پسند زمان  قرعه کشی جام باشگاه های فوتسال آسیا را رسماً اعلام کرد .\n",
      "\n",
      "\u001b[1mاعلام زمان قرعه کشی جام باشگاه های فوتسال آسیا\u001b[0m\n",
      "به گزارش خبرگزاری فارس، کنفدراسیون فوتبال آسیا (AFC) در نامه ای رسمی به فدراسیون فوتبال ایران و باشگاه گیتی پسند زمان  قرعه کشی جام باشگاه های فوتسال آسیا را رسماً اعلام کرد .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_data(id, query,phrase):\n",
    "    print('\\033[1m'+data[id]['title']+'\\033[0m')\n",
    "\n",
    "    text = data[id]['content']\n",
    "    sentences = my_tokenizer.tokenize_sentences(text)\n",
    "\n",
    "    arr = [0] * len(query)\n",
    "\n",
    "    flag_phrase = False\n",
    "\n",
    "    for j in sentences:\n",
    "        flag = False\n",
    "        \n",
    "        if len(phrase)!= 0 and phrase[0] in j and flag_phrase == False:\n",
    "            words = my_tokenizer.tokenize_words(my_normalizer.normalize(j))\n",
    "            \n",
    "            dele = []\n",
    "            for i in words:\n",
    "                if i not in stop_words and i not in punctuations:\n",
    "                    dele.append(i)\n",
    "            words = dele\n",
    "\n",
    "            stemmed = []\n",
    "            for i in words:\n",
    "                stemmed.append(myStem.convert_to_stem(i))\n",
    "            words = stemmed\n",
    "\n",
    "            for w in range(len(words)):\n",
    "                if phrase[0] == words[w]:\n",
    "                    context = 1\n",
    "                    for p in range(1,len(phrase)):\n",
    "                        if len(words)> p+w and words[p + w] != phrase[p]:\n",
    "                            context = 0\n",
    "                    if context == 1:\n",
    "                        print(j)\n",
    "                        for k in range(len(query)):\n",
    "                            if query[k] in j:\n",
    "                                arr[k] += 1\n",
    "                        flag_phrase = True\n",
    "                        flag = True\n",
    "                        break\n",
    "\n",
    "        if not flag:\n",
    "            for i in range(len(query)):\n",
    "                if query[i] in j and arr[i] == 0:\n",
    "                    print(j)\n",
    "                    for k in range(len(query)):\n",
    "                        if query[k] in j:\n",
    "                            arr[k] += 1\n",
    "                    break\n",
    "    \n",
    "    print('')\n",
    "\n",
    "\n",
    "show_data('7346', [],['دانشگاه','امیرکبیر'])\n",
    "show_data('0', ['گیتی','ایران'],[])\n",
    "show_data('0', ['گیتی'],['کنفدراسیون','فوتبال','آسیا'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['خوبی'], ['چطور', 'داری'], ['سلام', 'میگو'])\n",
      "([], ['چطور', 'داری'], ['سلام', 'خوبی', 'شعر', 'میگو'])\n",
      "(['چطور', 'خوبی'], [], ['سلام\\u200cداری', 'شعر', 'میگو'])\n"
     ]
    }
   ],
   "source": [
    "# supports almost all querys, throws exceptions when needed, the only type of query that it does not support is when there is a not inside of a phrase in a query\n",
    "# for example in the following phrase:\n",
    "# 'this is \"a phrase with a !not inside of it\" and its not supported'\n",
    "\n",
    "\n",
    "def phrase_split(input):\n",
    "    if '\"' not in input:\n",
    "        return ([], input)\n",
    "    phrase = []\n",
    "    nonphrase = []\n",
    "    if input.count('\"') % 2 == 1:\n",
    "        raise Exception(\n",
    "            'correct input, there is an extra \" somewhere in the phrase')\n",
    "\n",
    "    if '\"' in input:\n",
    "        input = input.split('\"')\n",
    "    \n",
    "    for i in range(len(input)):\n",
    "        if i % 2 == 0:\n",
    "            nonphrase.append(input[i])\n",
    "        else:\n",
    "            phrase.append(input[i])\n",
    "    str = ''\n",
    "    for i in range(len(nonphrase)):\n",
    "        str += nonphrase[i]\n",
    "    nonphrase = str\n",
    "    phrase = my_tokenizer.tokenize_words(my_normalizer.normalize(phrase[0]))\n",
    "    \n",
    "    return phrase, nonphrase\n",
    "\n",
    "\n",
    "def not_split(input):\n",
    "\n",
    "    notput = []\n",
    "    output = []\n",
    "    input = input.split(' ')\n",
    "    # print(input)\n",
    "    for i in input:\n",
    "        if len(i)!=0:\n",
    "            if i[0]=='!':\n",
    "                notput.append(i[1:])\n",
    "            else :\n",
    "                output.append(i)\n",
    "    str = ''\n",
    "    for i in output:\n",
    "        str+= i\n",
    "        str+= \" \"\n",
    "    output = str\n",
    "    output = my_tokenizer.tokenize_words(my_normalizer.normalize(output))\n",
    "        \n",
    "    return notput, output\n",
    "\n",
    "\n",
    "def full_splitter(input):\n",
    "\n",
    "    phrase, nonphrase = phrase_split(input= input)    #phrase is a list of strings inside double quotations #nonphrase is only a string with the remaining characters\n",
    "    \n",
    "    notput, rem = not_split(input= nonphrase)   #\n",
    "\n",
    "    phrase_new = []\n",
    "    notput_new = []\n",
    "    rem_new = []\n",
    "\n",
    "    if len(phrase)!=0:\n",
    "        for i in phrase:\n",
    "            remi = myStem.convert_to_stem(i)\n",
    "            if remi not in punctuations:\n",
    "                phrase_new.append(remi)\n",
    "    phrase = phrase_new\n",
    "\n",
    "    if len(notput)!=0:\n",
    "        for i in notput:\n",
    "            remi = myStem.convert_to_stem(i)\n",
    "            if remi not in punctuations:\n",
    "                notput_new.append(remi)\n",
    "    notput = notput_new\n",
    "\n",
    "    if len(rem)!=0:\n",
    "        for i in rem:\n",
    "            remi = myStem.convert_to_stem(i)\n",
    "            if remi not in punctuations:\n",
    "                rem_new.append(remi)\n",
    "    rem = rem_new\n",
    "\n",
    "\n",
    "    return phrase, notput, rem\n",
    "\n",
    "print(full_splitter(' سلام !چطوری \"خوبی؟\" !داری \"شعر\" میگویی'))\n",
    "print(full_splitter(' سلام !چطوری خوبی؟ !داری شعر میگویی'))\n",
    "print(full_splitter(' سلام \"چطوری خوبی؟\" داری شعر میگویی'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'7346': [3.4756465643105914], '7395': [2.6714576726855506], '7416': [2.6714576726855506], '7535': [2.6714576726855506], '7579': [2.6714576726855506], '7634': [2.6714576726855506], '8125': [4.279835455935632], '8463': [3.4756465643105914], '8645': [2.6714576726855506], '9261': [2.6714576726855506], '9286': [4.538726453746061], '9808': [2.6714576726855506], '9845': [2.6714576726855506], '10024': [3.4756465643105914], '10361': [2.6714576726855506], '10567': [3.9460669094077496], '10568': [3.4756465643105914], '10681': [2.6714576726855506], '11176': [2.6714576726855506], '11405': [3.9460669094077496], '11437': [2.6714576726855506], '11603': [4.7502558010327895], '11645': [2.6714576726855506], '11694': [2.6714576726855506], '11928': [2.6714576726855506], '12134': [2.6714576726855506]}\n"
     ]
    }
   ],
   "source": [
    "def extract_docs_with_query_words(query):\n",
    "    godDict = {}\n",
    "    for i in range(0, len(query)):\n",
    "        for doc in dic_tf[query[i]][1]:\n",
    "            if doc not in godDict:\n",
    "                godDict[doc] = len(query) * [0]\n",
    "            godDict[doc][i] = dic_tf[query[i]][1][doc][0]\n",
    "    return godDict\n",
    "\n",
    "print(extract_docs_with_query_words(['امیرکبیر']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def cos(query,doc):\n",
    "    sum = 0\n",
    "    sizeQuery = 0\n",
    "    sizeDoc = 0\n",
    "    for i in range(0, len(query)):\n",
    "        sum+=query[i]*doc[i]\n",
    "        sizeQuery += query[i]*query[i]\n",
    "        sizeDoc += doc[i]*doc[i]\n",
    "    return sum/(math.sqrt(sizeDoc*sizeQuery))\n",
    "\n",
    "print(cos([1,1,1,1], [1,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUR_K = 5\n",
    "\n",
    "def query(text):\n",
    "    \n",
    "    text = full_splitter(text)\n",
    "\n",
    "    phrase = text[0]\n",
    "    notput = text[1]\n",
    "    input = text[2]\n",
    "\n",
    "    # Searching query\n",
    "    docs = {}\n",
    "    null_input = False\n",
    "\n",
    "    if len(input) == 0:\n",
    "        input = phrase\n",
    "        null_input = True\n",
    "    \n",
    "    #TODO  \n",
    "    godDict = extract_docs_with_query_words(input)\n",
    "    top_k = {}\n",
    "    for doc in godDict:\n",
    "        similar = cos(len(input)*[1],godDict[doc])\n",
    "        if len(top_k) < OUR_K:\n",
    "            top_k[doc]=similar\n",
    "        else:\n",
    "            mini = 2\n",
    "            docID = ''\n",
    "            for i in top_k:\n",
    "                if top_k[i] < mini:\n",
    "                    mini = top_k[i]\n",
    "                    docID = i\n",
    "            if mini < similar:\n",
    "                top_k.pop(docID)\n",
    "                top_k[doc]=similar\n",
    "\n",
    "    print(top_k)\n",
    "\n",
    "    # Showing result\n",
    "\n",
    "    res = [i for i in top_k]\n",
    "    for i in range(len(input), 0, -1):\n",
    "        for j in docs:\n",
    "            if docs[j][0] == i:\n",
    "                res.append((docs[j][1], j))\n",
    "        if len(res) > 5:\n",
    "            break\n",
    "\n",
    "    rank = 1\n",
    "\n",
    "    res.sort(reverse=True)\n",
    "    print('There are '+str(len(res))+' results for your search\\n')\n",
    "    if len(res) > 5:\n",
    "        res = res[:5]\n",
    "    if null_input:\n",
    "        input = []\n",
    "    for i in res:\n",
    "        print(rank, 'There are '+str(i) +\n",
    "              ' matches for your search in this document')\n",
    "        rank += 1\n",
    "        show_data(i, input,phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'8998': 1.0, '9356': 1.0, '9564': 1.0, '9569': 1.0}\n",
      "There are 4 results for your search\n",
      "\n",
      "1 There are 9569 matches for your search in this document\n",
      "\u001b[1mتانک کرار در نیروی زمینی سپاه عملیاتی شد\u001b[0m\n",
      " علاوه بر این روی برجک تانک‌ تحویل‌داده به سپاه جایگاه نصب سیستم حفاظتی دفاع فعال برای مقابله با موشک‌ها و پرتابه‌های ضدزره است .\n",
      "\n",
      "2 There are 9564 matches for your search in this document\n",
      "\u001b[1mسومین روز رزمایش پیامبر اعظم ۱۷/ از بهره‌گیری از تسلیحات جدید تا قدرت‌نمایی تانک کرار\u001b[0m\n",
      "           حضور تانک کرار در رزمایش پیامبر اعظم 17            علاوه بر این روی برجک تانک‌ تحویل‌داده به سپاه جایگاه نصب سیستم حفاظتی دفاع فعال برای مقابله با موشک‌ها و پرتابه‌های ضدزره است .\n",
      "\n",
      "3 There are 9356 matches for your search in this document\n",
      "\u001b[1mتقدیر 160 نماینده مجلس از اقدام قاطع مرزبانی در دفاع از مرزهای شرقی\u001b[0m\n",
      " در این بیانیه آمده است؛ «بسمه تعالی محضر ملت شریف و بزرگ ایران احتراما از ساعت 10 صبح روز چهارشنبه تا ساعت 19 همان روز، نیروهای شجاع و دلاور مرزبان جمهوری اسلامی ایران، فرماندهی مرزبانی استان سیستان و بلوچستان، هنگ مرزی زابل، گروهان زهک در میل مرزی 54 تا 58 به طول 14 کیلومتر و پاسگاه شهید اسلامی، جانبازان و تعداد 6 برجک در دفاع از مرزهای میهن اسلامی و مرزنشینان عزیز در مقابله با درگیری و تیراندازی با عناصر طالبان به کشاورزان عزیز منطقه و خاک جمهوری اسلامی ایرا،ن به صورت دلاورمردانه و جانفشانی به مقابله پرداخته و با عملیات مقابله به مثل با سلاح های نیمه سنگین و سنگین، به مقابله پرداخته و با جواب دندان‌شکن و پاسخ قاطع موجب تلفات جانی و مالی و عقب‌نشینی آن ها شد .\n",
      "\n",
      "4 There are 8998 matches for your search in this document\n",
      "\u001b[1mگزارش سفر نمایندگان ویژه رئیس‌جمهور به مناطق سیل‌زده/بررسی خسارات و گفت‌وگو با مردم\u001b[0m\n",
      " وحیدی سپس از منطقه معروف به سه‌راه استانداری در ابتدای جاده برجک و منطقه محمدآبادکتِکی رودبار بازدید به‌عمل آورد .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query(\"برجک\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Champion's list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_CHAMPION = 3\n",
    "\n",
    "\n",
    "def replaceWithMin(arr, val):\n",
    "    minArr = arr[0]\n",
    "    id = 0\n",
    "    for i in range(0, len(arr)):\n",
    "        if minArr > arr[i]:\n",
    "            minArr = arr[i]\n",
    "            id = i\n",
    "    if minArr>val:\n",
    "        arr[id] = val\n",
    "    return\n",
    "\n",
    "\n",
    "championsList = {}\n",
    "for word in dict_tf:\n",
    "    dict_tf[word][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f5aca982b3795189cf78be42799eaa89b24ad2a4bf87aac4f9f0d78ad9ce65d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
